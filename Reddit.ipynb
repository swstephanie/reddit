{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77beb446",
   "metadata": {},
   "source": [
    "# PRAW\n",
    "https://praw.readthedocs.io/en/stable/\n",
    "\n",
    "</br>\n",
    "https://github.com/praw-dev/praw/\n",
    "\n",
    "# Pros\n",
    "- live metadata including comments and replies \n",
    "- \n",
    "\n",
    "\n",
    "# Cons\n",
    "### 1.The Reddit API has a limit of about 1000 items per listing, so you won't be able to get more than that with PRAW. \n",
    "> <h3>To get more posts, one option is PushShift.io, a third-party service that has more data on Reddit items. PushShift has an API wrapper called PSAW.</h3>\n",
    "> https://stackoverflow.com/questions/60340997/praw-python-doesnt-show-every-submission-from-a-subreddit\n",
    "\n",
    "\n",
    "### 2. For some posts, the replies are extracted as the comments before we look into the replies layer\n",
    "\n",
    "> \n",
    "\n",
    "\n",
    "# Updates\n",
    "\n",
    "### Get the link of the image\n",
    "- Done, we include the image in the post and the image link in the comments \n",
    "\n",
    "### Check the limit\n",
    "- the API limit is 100. Maybe we need to use BeautifulSoup to scrap the webpage\n",
    "\n",
    "### replace_more (100)\n",
    "- set the parameter to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29149a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install praw\n",
    "#!pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip\n",
    "import praw\n",
    "import json\n",
    "import time\n",
    "import config \n",
    "import re\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85084a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id= config.client_id,\n",
    "    client_secret= config.client_secret,\n",
    "    password= config.password,\n",
    "    username= config.username,\n",
    "    user_agent= config.user_agent,\n",
    ")\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd69c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following functions can be packaged in the config.py\n",
    "def try_else_None(sub_id):\n",
    "    submission = reddit.submission(sub_id)\n",
    "    try:\n",
    "        res = submission.media_metadata\n",
    "        return res\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def get_image_from_submission(sub_id):\n",
    "    submission = reddit.submission(sub_id)\n",
    "    if try_else_None(sub_id) is None:\n",
    "        return None\n",
    "    else:\n",
    "        img_url = []\n",
    "        for i in submission.media_metadata.keys():\n",
    "            img_url.append(submission.media_metadata.get(i).get('p')[-1].get('u'))\n",
    "        return img_url\n",
    "        \n",
    "def findURL(string):\n",
    "  \n",
    "    # findall() has been used \n",
    "    # with valid conditions for urls in string\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url = re.findall(regex,string)      \n",
    "    return [x[0] for x in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211a7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddits(subreddit = \"wallstreetbets\",limit = None):\n",
    "    #cannot find parameter for ALL \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_submissions = dict()\n",
    "    total_submissions['subreddit'] = subreddit\n",
    "    posts = []\n",
    "    for submission in reddit.subreddit(subreddit).top(limit=limit): \n",
    "        #print(type(submission.__dict__))\n",
    "        # print(submission.__dict__)\n",
    "        \n",
    "        #first, try to include some info about the submission (which refers to a post)\n",
    "        submission_dict = {\n",
    "            'id':submission.id,\n",
    "            'author':submission.author,#[submission.author.id,submission.author.name],\n",
    "            'url':submission.url,\n",
    "            'num_comments':submission.num_comments,\n",
    "            'permalink':submission.permalink,\n",
    "            'score':submission.score,\n",
    "            'upvote_ratio':submission.upvote_ratio,\n",
    "            'media_metadata':try_else_None(submission.id),\n",
    "            'selftext':submission.selftext,\n",
    "            'image':get_image_from_submission(submission.id)\n",
    "            \n",
    "        }\n",
    "        comments_list = []\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "#             if isinstance(comment, praw.models.MoreComments):\n",
    "#                 continue\n",
    "            comment_dict = {}\n",
    "            comment_dict['id'] = comment.id\n",
    "            comment_dict['body'] = comment.body\n",
    "            comment_dict['img_url'] = findURL(comment.body)\n",
    "            \n",
    "            try:\n",
    "                reply_list = []\n",
    "                for reply in comment.replies:\n",
    "                    reply_list.append({'id': reply.id,'body':reply.body})\n",
    "                    print(type(i), i.body, sep=' ')\n",
    "                \n",
    "                #comment_dict['replies'] = comment.replies\n",
    "                comment_dict['replies'] = reply_list\n",
    "                    \n",
    "                    \n",
    "                \n",
    "            except:\n",
    "                comment_dict['replies'] = []\n",
    "            comments_list.append(comment_dict)\n",
    "            \n",
    "        submission_dict['comments'] = comments_list\n",
    "        #print(json.dumps(submission_dict, ensure_ascii = False,indent=4, sort_keys=True))\n",
    "\n",
    "        posts.append(submission_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    total_submissions['posts'] = posts\n",
    "    \n",
    "    end_time = time.time()\n",
    "    if round(end_time - start_time) < 60:\n",
    "        print('elasped time is',round(end_time - start_time,2),'s')\n",
    "    else:\n",
    "        print('elasped time is',round((end_time - start_time)/60,2),'s')\n",
    "    return total_submissions\n",
    "    #return json.dumps(total_submissions, ensure_ascii = False,indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = get_reddits(subreddit = \"news\",limit = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a77bd",
   "metadata": {},
   "source": [
    "# Pushshift\n",
    "\n",
    "https://www.reddit.com/r/pushshift/comments/bcxguf/new_to_pushshift_read_this_faq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504650d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `search_comments` and `search_submissions` methods return generator objects\n",
    "gen = api.search_submissions(limit=100)\n",
    "results = list(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b475b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method GildableMixin.award of Submission(id='l8rf4k')>\n"
     ]
    }
   ],
   "source": [
    "for submission in reddit.subreddit('wallstreetbets').top(limit=None): \n",
    "    #print(type(submission.__dict__))\n",
    "    print(submission.award)\n",
    "    break\n",
    "\n",
    "    #first, try to include some info about the submission (which refers to a post)\n",
    "#     submission_dict = {\n",
    "#         'id':submission.id,\n",
    "#         'author':submission.author,#[submission.author.id,submission.author.name],\n",
    "#         'url':submission.url,\n",
    "#         'num_comments':submission.num_comments,\n",
    "#         'permalink':submission.permalink,\n",
    "#         'score':submission.score,\n",
    "#         'upvote_ratio':submission.upvote_ratio,\n",
    "#         'media_metadata':try_else_None(submission.id),\n",
    "#         'selftext':submission.selftext,\n",
    "#         'image':get_image_from_submission(submission.id)\n",
    "\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "792c3ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'![gif](emote|free_emotes_pack|no_mouth)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.comment('i5xm9rr')."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
